{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18c7b77-c69b-4436-aeb8-23ddcdfe00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f9479b0-7253-458f-83c4-44ceab5599c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import *\n",
    "from model import Generator\n",
    "from model_conv import BlondClassifier, GenderClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfb740-e85b-4f18-8021-8d37eba7189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/mnt/data/10708-controllable-generation/data/celeba/img_align_celeba'\n",
    "attr_fp = '/mnt/data/10708-controllable-generation/data/celeba/list_attr_celeba.txt'\n",
    "stargan_fp = 'stargan_celeba_128/models/200000-G.ckpt'\n",
    "male_classifier_fp = '../male_classifier/saved_models/epoch5.pth'\n",
    "blonde_classifier_fp = 'Blond.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c596d8c4-2d39-4d7f-949f-642718e5fa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing the CelebA dataset...\n"
     ]
    }
   ],
   "source": [
    "celeba_loader = get_loader(img_dir, attr_fp, \n",
    "                           ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'],\n",
    "                           178, 128, 32,\n",
    "                           'CelebA', 'test', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64048bf5-8dfd-4007-8831-487ef37750f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0763f18-7da2-4641-8678-d2a494c12f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_psnr(recon_imgs, imgs):\n",
    "    recon_imgs = recon_imgs.reshape((recon_imgs.shape[0], -1, 3))\n",
    "    imgs = imgs.reshape((recon_imgs.shape[0], -1, 3))\n",
    "    channel_wise_mse = ((imgs - recon_imgs) ** 2).mean(axis=1)\n",
    "    sample_wise_mse = channel_wise_mse.mean(axis=1)\n",
    "    mse = sample_wise_mse.mean()\n",
    "    mean_psnr = 10*np.log10(255*255/mse)\n",
    "    return mean_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74e5d477-ca96-4f02-82a6-1b6b483287a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_rmse(recon_imgs, imgs):\n",
    "    recon_imgs = recon_imgs.reshape((recon_imgs.shape[0], -1, 3))\n",
    "    imgs = imgs.reshape((recon_imgs.shape[0], -1, 3))\n",
    "    channel_wise_rmse = ((imgs - recon_imgs) ** 2).mean(axis=1) ** 0.5\n",
    "    sample_wise_rmse = channel_wise_rmse.mean(axis=1)\n",
    "    rmse = sample_wise_rmse.mean()\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0dc25-eb48-4271-adf0-4325eb0caf2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Blondes v/s Blacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77bb568-ef01-4b41-9f95-f339ecd7b2f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get StarGAN preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f5a6c2a-87fa-4320-80cc-00cc78cef193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "generator = Generator(64, 5, 6).to(device)\n",
    "generator.load_state_dict(torch.load(stargan_fp, map_location=device))\n",
    "# _ = generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765a102-e3bc-4158-978b-14e1a7c247b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blondes = []\n",
    "blacks = []\n",
    "blondes_orig = []\n",
    "blacks_orig = []\n",
    "for images, labels in tqdm(celeba_loader):\n",
    "    labels[:, 0] = 1 - labels[:, 0]\n",
    "    labels[:, 1] = 1 - labels[:, 1]\n",
    "    black_idxs = (labels[:, 0] == 1).nonzero()\n",
    "    blonde_idxs = (labels[:, 1] == 1).nonzero()\n",
    "    with torch.no_grad():\n",
    "        outs = generator(images, labels).detach()\n",
    "    blondes.append(outs[blonde_idxs])\n",
    "    blacks.append(outs[black_idxs])\n",
    "    \n",
    "    labels[:, 0] = 1 - labels[:, 0]\n",
    "    labels[:, 1] = 1 - labels[:, 1]\n",
    "    with torch.no_grad():\n",
    "        outs = generator(images, labels).detach()\n",
    "    blondes_orig.append(outs[blonde_idxs])\n",
    "    blacks_orig.append(outs[black_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "abb0dfd1-5ad4-4e49-84d4-c96c25c16f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blo = torch.cat(blondes, dim=0)\n",
    "bla = torch.cat(blacks, dim=0)\n",
    "ori_blo = torch.cat(blondes_orig, dim=0)\n",
    "ori_bla = torch.cat(blacks_orig, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b562dfd-a786-41d4-ac91-3e95d23f4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(blo, 'blondes.pth')\n",
    "# torch.save(bla, 'blacks.pth')\n",
    "# torch.save(ori_blo, 'blondes_orig.pth')\n",
    "# torch.save(ori_bla, 'blacks_orig.pth')\n",
    "\n",
    "blo = torch.load('blondes.pth')\n",
    "bla = torch.load('blacks.pth')\n",
    "ori_blo = torch.load('blondes_orig.pth')\n",
    "ori_bla = torch.load('blacks_orig.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e2d51-58ef-4573-b36d-69a3b15337ee",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ed9b0bb-5070-444c-be6e-e458ea4a46d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "blond_classifier = BlondClassifier()\n",
    "blond_classifier.load_state_dict(torch.load(blonde_classifier_fp, map_location='cpu'))\n",
    "_ = blond_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c375c862-b5a8-40e9-989d-7036aa0ed44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blo_resized = torchvision.transforms.Resize(64)(blo.squeeze())\n",
    "bla_resized = torchvision.transforms.Resize(64)(bla.squeeze())\n",
    "ori_blo = torchvision.transforms.Resize(64)(ori_blo.squeeze())\n",
    "ori_bla = torchvision.transforms.Resize(64)(ori_bla.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6ea009a-c082-4e8d-885c-bbef8aa83e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_blo = torch.utils.data.DataLoader(list(zip(blo_resized, ori_blo)), batch_size=32, shuffle=False)\n",
    "loader_bla = torch.utils.data.DataLoader(list(zip(bla_resized, ori_bla)), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abc9a800-bc31-41b3-9247-9c88a432a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnt = 0\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d527b56b-9cd3-4af0-8e27-97b0f8ab73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch_ori in loader_blo:\n",
    "    blo_preds = blond_classifier(batch)\n",
    "    blo_classes = blo_preds.argmax(dim=1)\n",
    "    total_cnt += blo_classes.shape[0]\n",
    "    correct += (blo_classes == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "257484eb-da20-41af-9b6c-b240a43e53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch_ori in loader_bla:\n",
    "    bla_preds = blond_classifier(batch)\n",
    "    bla_classes = bla_preds.argmax(dim=1)\n",
    "    total_cnt += bla_classes.shape[0]\n",
    "    correct += (bla_classes == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "933c3b97-f562-43f6-a7d1-a53b8cd40ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6235)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37973297-1efa-4186-801f-bea3ebf5d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch_ori in loader_blo:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d0ec1c1-fefa-42d8-bfb9-6478327de136",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(torch.cat((denorm(batch_ori[[4,1]]), denorm(batch[[4,1]])), dim=0), 'tmp.png', nrow=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0b75ed-ef89-4afc-9a72-c36342d49e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [00:37<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "recon = []\n",
    "orig = []\n",
    "for images, labels in tqdm(celeba_loader):\n",
    "    labels[:, 0] = 1 - labels[:, 0]\n",
    "    labels[:, 1] = 1 - labels[:, 1]\n",
    "    with torch.no_grad():\n",
    "        outs = generator(images.to(device), labels.to(device)).detach()\n",
    "    labels[:, 0] = 1 - labels[:, 0]\n",
    "    labels[:, 1] = 1 - labels[:, 1]\n",
    "    with torch.no_grad():\n",
    "        outs = generator(outs.to(device), labels.to(device)).detach()\n",
    "    orig.append(images)\n",
    "    recon.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a7fff4a-68e0-45a6-9001-7aef57b03585",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_all = torch.cat(recon, dim=0).to(device)\n",
    "orig_all = torch.cat(orig, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed782c5e-adac-4c9d-a1d8-588396d5817e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.44711589072598"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_psnr(recon_all.cpu().numpy(), orig_all.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852928d5-e035-4628-ab32-1fd5c66e995d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Males v/s Females"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d6c67-c76b-424e-860f-1cba3fe57ca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get StarGAN preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02eb6f2e-4c2a-481d-bb19-75eef0d3ad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "generator = Generator(64, 5, 6).to(device)\n",
    "generator.load_state_dict(torch.load(stargan_fp, map_location=device))\n",
    "# _ = generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5959ee2e-c928-4d33-85b3-3bdd5382c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [00:39<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "males = []\n",
    "females = []\n",
    "males_orig = []\n",
    "females_orig = []\n",
    "for images, labels in tqdm(celeba_loader):\n",
    "    labels[:, -2] = 1 - labels[:, 0]\n",
    "    male_idxs = (labels[:, -2] == 1).nonzero()\n",
    "    female_idxs = (labels[:, -2] == 0).nonzero()\n",
    "    with torch.no_grad():\n",
    "        outs = generator(images.to(device), labels.to(device)).detach()\n",
    "    males.append(outs[male_idxs])\n",
    "    females.append(outs[female_idxs])\n",
    "    \n",
    "    labels[:, -2] = 1 - labels[:, -2]\n",
    "    with torch.no_grad():\n",
    "        outs = generator(images.to(device), labels.to(device)).detach()\n",
    "    males_orig.append(outs[male_idxs])\n",
    "    females_orig.append(outs[female_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92e10d5d-2525-476f-9f80-8e7215c9b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = torch.cat(males, dim=0)\n",
    "fe = torch.cat(females, dim=0)\n",
    "ori_ma = torch.cat(males_orig, dim=0)\n",
    "ori_fe = torch.cat(females_orig, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd50605a-bb6b-4205-8dc3-084f57d1eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(ma, 'males.pth')\n",
    "# torch.save(fe, 'females.pth')\n",
    "# torch.save(ori_ma, 'males_orig.pth')\n",
    "# torch.save(ori_fe, 'females_orig.pth')\n",
    "\n",
    "ma = torch.load('males.pth')\n",
    "fe = torch.load('females.pth')\n",
    "ori_ma = torch.load('males_orig.pth')\n",
    "ori_fe = torch.load('females_orig.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9454d4-f9e3-487f-a680-f73a5b31b85c",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4656c6d-0add-4dc4-8ed3-87471d84021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_classifier = torch.load(male_classifier_fp).to(device)\n",
    "_ = male_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "609657a1-1de1-4911-ab9f-8f3094a4773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_resized = torchvision.transforms.Resize(64)(ma.squeeze())\n",
    "fe_resized = torchvision.transforms.Resize(64)(fe.squeeze())\n",
    "ori_ma = torchvision.transforms.Resize(64)(ori_ma.squeeze())\n",
    "ori_fe = torchvision.transforms.Resize(64)(ori_fe.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16d9e32e-e0b2-431a-a44e-e3266d0d7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_ma = torch.utils.data.DataLoader(list(zip(ma_resized, ori_ma)), batch_size=32, shuffle=False)\n",
    "loader_fe = torch.utils.data.DataLoader(list(zip(fe_resized, ori_fe)), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8a2d9f1-176d-4b27-a321-e3bbaba43e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnt = 0\n",
    "correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3c660e7-83d7-405f-bc1f-51dabcac4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch_ori in loader_ma:\n",
    "    ma_preds = male_classifier(batch)\n",
    "    ma_classes = ma_preds.argmax(dim=1)\n",
    "    total_cnt += ma_classes.shape[0]\n",
    "    correct += (ma_classes == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7d5b78e-5550-4e38-8731-0249c2c25d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch_ori in loader_fe:\n",
    "    fe_preds = male_classifier(batch)\n",
    "    fe_classes = fe_preds.argmax(dim=1)\n",
    "    total_cnt += fe_classes.shape[0]\n",
    "    correct += (fe_classes == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c47991f6-fddd-4e51-a530-3dd4e023a5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7449, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cce8410b-1dab-4f97-8c2f-f3f4210c03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch_ori in loader_ma:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05e0fdce-be61-4fdc-bb3a-ad770b648780",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(torch.cat((denorm(batch_ori[[4,1]]), denorm(batch[[4,1]])), dim=0), 'tmp.png', nrow=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "224835f8-2d0c-4766-979a-4cfef4abe31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [00:37<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "recon = []\n",
    "orig = []\n",
    "for images, labels in tqdm(celeba_loader):\n",
    "    labels[:, -2] = 1 - labels[:, -2]\n",
    "    with torch.no_grad():\n",
    "        outs = generator(images.to(device), labels.to(device)).detach()\n",
    "    labels[:, -2] = 1 - labels[:, -2]\n",
    "    with torch.no_grad():\n",
    "        outs = generator(outs.to(device), labels.to(device)).detach()\n",
    "    orig.append(images)\n",
    "    recon.append(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32edab64-8b47-472f-a78a-79c8b1c6442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_all = torch.cat(recon, dim=0).to(device)\n",
    "orig_all = torch.cat(orig, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85d43ce3-15fb-43e4-b15b-29e411feddfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.04880108580704"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_psnr(recon_all.cpu().numpy(), orig_all.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef290be-9f70-4b33-a7bd-47ba03f0534d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
